
Peter Coffee
In every field other than diginomics, an expert in “protocol” is kept around to avoid hurt feelings and to help people get stuff done.
“With the evolution of globalization, protocol has become a highly sophisticated and strategic asset in today’s business, military, and diplomatic world,” observes the web site of The Protocol School of Washington. “Leaders will count on you to advise and guide them in customs and cultural differences.” In our increasingly connected world of digital signals and services, that’s an appealing idea.
Unfortunately, in the present-day digisphere, protocol doesn’t accommodate differences. It denies them. A protocol like HTTP or IPMI is at best a “do this or be ignored”; at worst a “say it like this, or be dangerously misunderstood.” Hand something a bit string that you meant to represent a big-endian value, when that something was expecting a little-endian, and pretty much anything can happen…unless.
Unless what?

Unless a system is engineered, not merely constructed
Unless a system is designed with an eye toward common failure modes, with thought given to their quick and accurate detection and recognition
Unless the inevitability of failures is accepted, not dismissed as an implied confession of incompetence – and mitigation of likely failures is treated as a matter of professional ethics
Unless a system is envisioned in its end-use environment, with reasonable expectations on what might happen – and what might need to be avoided or accommodated after typical human (or other) errors.

We can see the difference between the merely prescriptive, “this is how to do it” protocol and the proactive, “this is how to make sure it won’t blow up” protocol in even the simplest situations. The so-called “Murphy’s Law” is widely mis-stated as “anything that can go wrong, will go wrong,” but this version ignores the most important lesson of the origin story.
What actually happened, back in 1949, was an incorrect assembly of a device that was equally easy to install in two different ways. This later led Edward A. Murphy Jr. to observe, in a moment of calm, “If there are two or more ways to do something, and one of those ways can result in a catastrophe, someone will do it.”
Even in the original moment, Murphy’s less politely phrased remark was not a novel observation. In 1877, an engineering society was warned that, “The human factor cannot be safely neglected in planning machinery. If attention is to be obtained, the engine must be such that the engineer will be disposed to attend to it.” If you want oil added before the bearings burn out, put the oil pressure gauge where people will see it; put the dipstick where people can reach it; design the cap for the add-oil opening so people can reach it, and open it, even when the engine is hot.
Bringing it up to date
What’s the modern-day equivalent of measures such as these?
In a world of mechanical/analog systems, there wasn’t much room for improvement. Even seven decades later, in Murphy’s situation, pretty much the most that could be done was to design a connector that only went together in one direction. Of course, someone would still need to install each half of the connector on bare wires correctly, so this would merely relocate the possible error site: we have to look for the place where mass-production efficiencies, and ideal circumstances of human attention and opportunities for testing, will make potential errors least threatening. (Note that Murphy, in particular, may have irritably turned down a chance to check the wiring in the field, before the correctable error became a costly mistake.)
It’s critical that we avoid jumping prematurely to a belief that something is being done often enough, routinely enough, that it no longer needs explicit measures for test and verification. The recent loss of a SpaceX rocket, for example, seems to have resulted from a simple strut that may have failed at substantially less than its specified load. Individual testing of those struts would have been a trivial cost compared to the loss of that rocket’s payload and the need to repeat that launch. Full kudos to SpaceX for driving rocketry from an economics of custom-build toward an economics of mass production, but that’s a line that you don’t want to cross in your mind before reality agrees.
Today, though, we can (and tomorrow we must) build systems that have enough rudimentary intelligence to ask “Are you sure about that?” In ancient times, putting a voltmeter on a pair of terminals with the probes reversed might literally bend the needle on the meter as it tried to move to the left instead of the right. Today, it’s more likely to display a polite “-” sign in front of the number, but it could actually do even better: if I’m testing a battery and the reading is “-1.5” volts, I’ve probably mis-positioned the probes, but “-0.03” volts might trigger a warning of “Probes reversed? Possible polarity reversal.” If I don’t know what that means, even a fairly cheap meter might today be able to tell me by pulling the explanation from the cloud.
I’m talking about systems that know what’s normal, and know enough about the abnormal to offer assistance when things aren’t right.
SpaceX
We need to get to work, quickly, on the next generation of such facilities, as we move with startling speed toward all manner of autonomous systems – whose mistakes may be much less reversible than overheating (or cooling) your house or mis-“correcting” your messages. A few months ago, I shared with my Facebook friends a report that “I’m sitting in a presentation on Internet of Things, noting the presenter’s earnest admonition that this needs to be done ‘securely and efficiently,’ with a footnote on his slide that says ‘[Company] takes no responsibility for any errors or omissions.’ In a world that feels a need to disclaim the risks of a PowerPoint slide, how ready are we for connected devices executing real-world autonomous processes?”
We can see some hopeful signs in work like the MIT “CodePhage” effort to develop self-repairing systems that can fix security bugs by borrowing from other implementations – even without access to their source code. Extended and generalized, this is a profoundly important step toward more reliable and resilient behavior – but even in domains that have been around for quite some time, such as email, we aren’t quickly converging on protocols that give me confidence in proper handling of imperfect reality.
In one recent case, for example, I saw email delivery failure responses that included “Mailbox unavailable”; “User account is over quota”; “Mailbox temporarily disabled”, and “User account is unavailable,” all with the same error code 550. Each of these four situations invites a different “what next?” – but should we need natural-language understanding of error messages, and protocol-officer skills in writing them, to handle even such a simple thing correctly?
The difference between the casual and the correct is not just a matter of words. The casual version of Murphy’s law blames malicious (or at least, uncaring) fate. The correct statement puts the onus on engineers to prevent catastrophe with thoughtful design. This often requires that human users be considered as part of the system; it always demands that partial failures result, not in a brittle and perhaps destructive collapse, but in some degree of useful partial function (if possible) with useful clues to the failure mode (and means of correction or workaround).
When I compare the contemporary model of “service provider” (for example, a Platform as a Service) against the legacy model of “technology vendor,” I’ve been known to say that traditional IT companies leave a box of razor blades on your doorstep – with instructions that say, “be safe.” A service provider, dependent on the future revenue stream of the active and cheerfully renewing user, simply has different incentives to offer (for example) security consultations and other “part of the service” capabilities for those with demanding requirements.
This needs to become the norm, in business practice as well as in the technology that puts our practices into effect. To borrow a phrase, “from best practices to next practices”: we’re talking about good manners, and that’s really what a “protocol” should encode.
Read more on: CRM and customer experienceDigital and content marketingInfrastructureInternet of ThingsPartner ZoneSalesforce 