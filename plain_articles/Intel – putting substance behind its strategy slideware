
It’s never been easy being an arms merchant to the household names supplying technology to the masses. No group knows this better than the executives at Intel, which dating back to the proto-PC era, played second fiddle, first to Microsoft, then to an array of online service providers using its products to build the things people actually work with.
To stand out from all the other silicon slingers, Intel used aggressive branding (“Intel inside”), advertising (disco-dancing bunny suit workers), its massive IP portfolio and legal wrangling (see its long, tired history with AMD) to establish itself as the premier provider of computing technology to both consumers and businesses.
Intel no longer directs its marketing at the general public, which reflects its focus on cloud builders and enterprise IT, however, the company is just as aggressive at making sure people know that Intel is the brains behind much of what makes Amazon, Facebook, Google, Microsoft, Netflix and other familiar services tick. That message was on full display during the latest edition of Intel’s Data-Centric strategy events.
The post-PC era posed an existential threat to Intel, but a concomitant rush to online consumer services, notably social networks, video streaming and mobile app backends, along with enterprise adoption of SaaS and now IaaS has fuelled explosive growth in its data center business. With that market under attack on several fronts including AMD (traditional CPUs), NVIDIA (AI accelerators), a host of startups like Ampere, Graphcore, Habana and others (ARM server CPUs, AI accelerators) and even the cloud builders themselves (AWS, Microsoft, Google and Facebook all have custom silicon under development and in production), Intel must contend with a component market that has splintered into a dozen pieces.
The company’s response has been a strategy that significantly expanded its scope from supplying CPUs and motherboards for traditional servers to every other corner of the data center, with products targeting storage, network services, IoT devices and infrastructure and the emerging market for AI acceleration systems. Here’s how I put it after Intel’s big strategy reveal last August:
Intel’s latest strategy, as evidenced by the event’s title, is all about data: processing it (CPUs), moving it (networking), storing it (SSDs and alternatives) and analyzing it (AI). … Intel executives have now translated the move from PC- to data-centrism into some concrete estimates of the business and revenue opportunities ahead.
As I said at the time:
The real difficulty Intel faces is that its current strategy runs the risk of being so broad that it becomes impossible to focus in all areas. Intel’s repeated stumbles in handling delivery of the 10nm part have left the market jittery while allowing competitors to steal a march. … Nevertheless, it will be interesting to see how Intel performs this high wire act.
Consider this week’s data-centric innovation day Intel’s first midterm, where it certainly gets an “A” for effort by announcing a slew of new products spanning computing, application acceleration, storage and networking designed for both large data centers and remote edge locations. How Intel executes these product introductions will determine its final grade, but it took pains to show that some tech-savvy major customers like what they’ve seen so far. Indeed, Intel emphasized the products’ relevance to real-world problems by parading a cavalcade of big customers, including the three mega cloud operators to share their predictably glowing experiences.

Source: Intel presentaton
Servers remain the foundation of Intel’s portfolio
Intel led the product blitz with what continues to be the centerpiece of its data-centric strategy, the next-generation of Xeon Scalable processors known as Cascade Lake. These have been on Intel’s roadmap for months and were confirmed last August. Expected to ship in Q4 of 2018, and likely available to the cloud builders since then, the second-generation parts are finally generally available.
The last few Intel  processor updates have been underwhelming once is tick-tock strategy, in which processor updates feature either a new manufacturing process or microarchitecture, collapsed under the weight of exploding costs in dealing with semiconductor physics at the molecular level and the design complexity of multi-billion transistor devices. Instead, like its predecessor Skylake parts, Cascade Lake includes a bevy of incremental features that add up to significant performance boosts, particularly for increasingly important AI, data analysis and networking workloads.
The highlights include:

Product SKUs with up to 28 cores per die or 56 cores in a 2-die multichip package (Cascade Lake-AP) and supporting 2-, 4- or 8-socket systems.
Support for more memory including 16 Gb DDR4 DRAM DIMMs and…
Intel’s new Optane DC persistent memory modules with up to 36 TB of non-volatile system memory (more on these below).
Faster turbo boost speeds of up to 4.4 GHz and..
A Speed Select feature that allows pinning applications to particular cores with higher boost frequencies to improve the performance of single-threaded applications, a feature Intel believes will appeal to network service providers delivering virtual network services (NFV).
A new Deep Learning Boost (DLBoost) feature to significantly improve the performance of deep learning model inference calculations by extending Intel’s AVX-512 instruction set for vector math to support reduced precision, 8-bit (INT8) calculations (a technique also supported by NVIDIA Tensor Cores).


Source: Intel presentation
The use of Optane storage, which occupies a price/performance niche between DRAM and flash, for system memory has long been expected. While Intel delivered the first such memory modules to Google last summer, it announced wider availability of Optane persistent memory at its Data Centric event. As a refresher, Optane is Intel’s brand of nonvolatile memory using the 3D Xpoint resistive storage technology (as opposed to the charge-trapping transistor technology used in flash memory) that Intel jointly developed with Micron Technologies. Its selling point has been lower latency and higher IOPS than flash while offering a price and non volatility that DRAM can’t match. Optane initially shipped as storage replacement or supplement for SSDs and is now available with a DRAM interface and micro-controller making it suitable for system memory.
Sadly for Intel, in the four years it’s taken Optane to go from announced technology to shipping memory module the capacity and pricing of DRAM has improved to the point that the performance gap Optane sought to fill is narrow. Indeed, Intel only expects to achieve a 20%  price-performance advantage over DRAM, with only modest performance improvements for most applications. While Intel touted the obvious success stories, like SAP using Optane modules to increase capacity and performance of its Hana in-memory database by an order of magnitude, other situations like HyperV VM servers can expect bumps closer to 30% .
Intel in your network
Cascade Lake is squarely designed for the data center, with many SKUs and features designed for cloud operators in particular. However, Intel sees the impending distribution of workloads to edge locations being catalyzed by the need to process massive streams of 5G wireless traffic, IoT data and streaming video content. In response, it updated its product line of lower power, more highly integrated processors designed for remote locations.
Intel’s product announcements targeting edge computing and telecom workloads include:

A performance bump to the lower-power Xeon D-series (D-1500→ D-1600) integrated SoCs that includes support for more memory and Intel’s Quick Assist crypto acceleration.
A new 100 Gbps Ethernet 800 NIC with a proprietary Application Device Queue (ADQ) feature that can filter packets into application-specific hardware queues to prioritize traffic for things like real-time communications and other time-sensitive applications. Intel claims that ADQ can increase throughput by 30 percent and reduce latency by 45 percent for I/O-intensive applications like database caches.

As mentioned, several features of the second-generation Scalable-series Xeons are designed to accelerate the performance of networking throughput and network virtual functions. These are particularly important to Intel’s future as networks become virtualized and run on conventional servers that can handle various applications instead of fixed-function hardware devices.

Source: Intel presentation
 My take
Intel is executing on a strategy to expand its TAM into all facets of the data center and beyond, including burgeoning needs to AI acceleration and telecom equipment at the edge. Many of the new features in the second-generation Scalable processors target cloud workloads, which coupled with on-stage appearances from the three major cloud providers showed that Intel understands who is responsible for its growth.

Source: Intel presentation
Intel also showed that it is not ceding the AI market to NVIDIA by incorporating deep learning acceleration into its mainstream processors. Indeed, the efficiency of DLBoost paired with the flexibility of a standard x86 instruction set and software support via the Intel-supported OpenVINO toolkit offers a compelling and cost-effective alternative to GPUs.
Intel faced a serious credibility problem after repeated delays in its 10nm process, and while it was careful to avoid any mention of its manufacturing imbroglio, the company showed that the execution problems haven’t spread to product development. Here’s hoping that Intel’s next mid-term exam, i.e. Data-Centric event, can share progress on manufacturing execution and demonstrate that it can mass produce products that deliver on its data center strategy.
 
Image credit - Artificial intelligence (AI), data mining, expert system software by @Jirsak, from Shutterstock.com.Read more on: Cloud platforms - infrastructure and architectureInfrastructure 