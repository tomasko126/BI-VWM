

When thinking about technical strategies for organizations, the work is normally broken down into:

data (after all that is what computing consumes to develop results),
infrastructure (without infrastructure you’re just using your imagination),
processes that the business uses to put structure around its actions,
applications that pull it all together into a common framework.

This has been the traditional approach and this simplified model may not be up to the task of getting organizations where they need to go – in the future. This thought caused me to think about the past and the potential for our future.
When I think about the use of computers in business there have been a number of phases. Each of these phases lasts somewhere between 16 and 20 years (they seem to be accelerating a bit though).

The mainframe stage – where the computer was worshiped behind closed doors by a small group of zealots. It was also where business really began to look at their information analytically. This lasted from the mid-50s to the mid-70s.
Personal computers – This started with a small revolt taking place and companies like DEC, HP, Data General came on the scene and introduced a different more departmental approach to computing. It then became even more personal when organizations like Dell, Apple and Compaq began to make a difference and computing became distributed. Lasting from the mid-70s through the early 90s.
Networked computing – This peaked during the dot com era was clearly one most people think of as a period of innovation and growth, with the birth of new approaches to business (like E-bay and Google). This phase ran though the late 200x
Digital business – Many think we are at the beginning of this phase but in reality we’re almost half-way through (based on the duration of previous waves), with concepts like IoT, sensing and mobile clearly in the mainstream of thought. There is still a great deal of experimentation underway and room to grow.

After I made that chart I realized it is really illustrating my perception of disruptive innovation and its impact on the perception of value, not just pure value generation. This illustration was based off analysis from Forrester Research back in 2008, assessing the waves of technology innovation and adoption up to that point.
It is about time to ask about what is the next phase? Each of these previous phases have orbited around hardware advances – will that be true in this more cloud-oriented, virtualized world? If not, what will be at the core of change?
It is possible with projects like HP’s The Machine or Quantum computing that a real hardware shift will be in the wind in the coming years. I think it more likely that the phases oriented around hardware will be overcome as Moore’s law slows.  We’ll instead focus more on a contextual understanding of the environment around us — what that deeper understanding means to our processes and methods.
As business relationships become more complex and wonderful, the need to quickly understand the nuances of the corners and the complexities of interrelationships will dominate. The devotion to time to action and attention engineering (maximizing the value of the scarce resource we call attention) will increase the need to use our computing, data and IP abundance in new and different ways.
I was talking with a groups of developers and mentioned that our current approach to hand-crafting solutions using 3rd generation languages may just not be up to the task. These techniques will still be important, but may no longer be in the center of value generation. We should be able to use the abundance of computing capabilities in whole new ways – including the process of generating computing solutions, hiding some of the complexities (of parallel processing and distributed/hybrid computing) so mere mortals can understand and make decisions.
For businesses, this will be a great opportunity to think outside the box of what is abundant and scarce of them. For IT, it is an opportunity to show value potential in a context the business will understand and break out of the scarcity model that has limited what applications can do historically. New techniques for predictive analytics and modeling will consume that abundance and open up new ways for people to understand and act upon all that information being collected. For technologists this is an opportunity to shape the future, to look at the signposts along the road and understand where you could go.
What do you think will be next?
Images via Charlie Bess

Read more on: Infrastructure 