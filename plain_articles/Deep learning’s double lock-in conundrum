
A couple of years ago, machine learning suddenly started appearing on the horizon in enterprise software. Systems of engagement ceased to be the new frontier of innovation as ‘systems of intelligence’ rapidly became all the rage. Nowadays, no self-respecting enterprise software vendor can be seen to be without a strategy for applying artifical intelligence to their systems.
These AI-enriched, cloud-based systems promise a higher level of automation and productivity by discovering patterns in past behavior and then acting on them when the same conditions reoccur. Benefits are promised across every kind of enterprise function — whether it’s raising the success rate of salespeople, improving collections, planning projects more efficiently or fixing defective equipment before it fails.
Vendors are rushing to embrace AI in part because they want to avoid getting locked out of an accelerating AI arms race. This is the law of AI lock-in, as entrepreneur Azeem Azhar explained last year:
As soon as AI gets baked in to the quality of product and its commercial success, the CEO has an ineluctable rationale for investing more in the AI. This creates a lock-in. Competitors need to respond. Because without AI you are nowhere …
This has happened in several key industries, such as video games and internet search, where you now simply cannot compete unless you have world-class AI. And [this] is spreading to more sectors: robotics, defence, finance and more.
AI acceleration
At the same time, AI itself is rapidly becoming more potent, driven by several converging factors that Azhar’s essay alludes to and which Kevin Kelly described in a Wired article in 2014:

Cheap parallel computation. Cheap graphics processing units (GPUs), originally developed to speed up visual effects in games, are equally efficient at powering neural nets, the basic building blocks of modern artificial intelligence. This younger processing technology still obeys Moore’s Law, doubling performance every 18 months or so. That’s without considering any further leaps that may be possible with further processor innovation — Google recently spoke about its Tensor Processing Unit, which powered AlphaGo’s machine intelligence triumph over humanity’s leading Go player, and which the company says improves performance by an order of magnitude — that’s seven years’ worth of Moore’s Law in one jump.
Big data. Data is the raw material from which machines learn, and the more of it they can access, the faster they will learn. Tumbling storage costs, new ways of storing large datasets, and cloud computing’s ability to connect to vast data resources all combine to provide a growing pool of raw material that fuels the ongoing explosion in machine learning.
Better algorithms. Since 2006, when AI researcher Geoff Hinton found a way to stack neural nets that he called ‘deep learning’, AI has taken off. Progress is propelled by the collaborative mechanisms that now exist on the Internet for sharing and building on all the advances made by individual AI researchers. People can easily share their experiments and learnings, exchange enhancements and progress faster as they refine their AI algorithms.

These factors are all raising the costs of participating in the AI revolution. Any investment in processing and storage infrastructure rapidly becomes obsolete. The science is constantly advancing too, which means the software and algorithms age equally rapidly.
For fear of being left behind, many application vendors are teaming up with cloud platform providers — Amazon, Microsoft, Google, IBM and others — to help keep pace with developments. Others are turning to smaller specialist providers. In doing so, they’re betting that the pooled resources of a shared platform will allow innovation at a far faster rate than an individual vendor could support alone.
Similarly, their enterprise customers hope that the vendors they put their trust in will not only deliver the benefits of AI but also continue to advance their AI capabilities in lock-step with the progress in the market as a whole.
Double lock-in
This is where the double lock-in comes into play. Machines learn, as mentioned earlier, by detecting patterns in the data they encounter. You hand over your data, they find patterns, and then they refine their learning over time. You may own the data, but who owns the learning? There are no standards for exporting and exchanging the results of machine learning. Once a vendor has committed to a third-party AI platform, once an enterprise has committed to a vendor’s AI-powered application, they are doubly locked in.
This double lock-in conundrum will sow the seeds of disillusion with the current hype over AI in enterprise software. After early successes, enterprises will suddenly discover they are locked into algorithms that were once leading-edge but have become primitive in comparison to the new state-of-the-art. The less successful AI providers will find their revenues flagging and fail to keep up the flow of investment required to keep their infrastructure up-to-date.
Buyers will have to choose between remaining loyal to a failing provider or going back to the drawing board with a new machine learning platform and starting over from scratch. There will come a point where the performance delta is so great that it will make sense to leap a generation, but at what competitive cost in the meantime?
These risks can be mitigated by relying as frequently as possible on open-source AI frameworks, or by relying on APIs when working with third-party providers. The risk will come when the AI platform itself takes responsibility for collecting and analyzing the data (as is common, for example, in Internet of Things solutions, as well as most SaaS offerings). In those cases, until some kind of standards emerge for exchange of machine learning metadata, the lock-in will be severe, curtailing the ability to progress beyond the first generation of AI investment.
My take
AI buyer, beware.
Image credit - Chained business handshake © Andrey Popov - Fotolia.comRead more on: Machine intelligence and AI 