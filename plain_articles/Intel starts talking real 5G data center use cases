
2020 is still ages away isn’t it? Well, no, it is less than 18 months away – as noted here! It’s also when 5G wireless communications services will really start in service. Gigabit data transfer capabilities will bring significant changes in how enterprises manage their data, and what they start to do with it.
Two disparate events over the last few weeks have managed to dovetail together nicely around the subject of 5G telecoms, with one indirectly highlighting an important end-user requirement and the other some of the architectural changes that are now needed to fulfil that goal.
The user requirement is customer experience and satisfaction, and emerged in a demonstration by Dell-EMC of some new workstation servers and laptops specifically targeted at the engineering design market, and in particular civil engineering and architecture.
Smaller, but more powerful servers are, as one might expect, providing designers with far richer capabilities, while more power and smaller laptops means that more of the actual design work can be done in direct collaboration with customers. For an architect, being able to offer customers a chance to participate in the live design work and view 3D walkthroughs in their office or front room has object selling advantages.
The current key weakness in this process, however, is the current state of communications, where 4G mobile capabilities are by no means widespread, and access via landline phone can only be relied upon to provide low bandwidth access. As the company’s UK Sales Lead for Workstation Systems, Mike Guinan, observed:
The distributed server-based model has always been wanted by customers, but it has never really delivered because of technology limitations. Now the hardware and software technologies can deliver and, with the coming of 5G communications, so can the communications networks. The cost/performance is now coming right, and architectural clients can now be involved in the on-going design process, and get real-time information on factors such as cost variations as design changes are made.
So now make it happen
Fulfilling such a capability is coming along faster than had been expected,  as a recent conversation with Alex Quach, Vice President, Network Platforms Group, General Manager, 5G Strategy and Program Office, made clear. He may have one of the longer job titles in the industry, but he is pretty close to the sharp end of delivering both the components to make 5G mobile happen, and the systems architectures need to deliver those valuable business results.
According to Quach, the key releases of the specifications for 5G are now pretty much locked down, and vendors are now building the necessary components, systems and tools that will make it happen. He expects to see the first commercial services being unveiled before the end of 2019.
Trials have been going on in China, South Korea, America and the UK, with the focus on getting gigabit per second bandwidths and what the practical issues are with operations in different terrains such as penetrating walls, what is the most acceptable type of rain and accessibility through trees and foliage. This will decide issues such as how to plan out cell tower coverage.
The first use cases will be based around mobile phone applications, but for the CIO, 5G is about much more than that. A bit further out will come use cases, such as the ability to connect very large numbers of machines together and provide the platform for introducing analytics and AI services that can significantly develop and expand the capabilities of the enterprise.
Given the multi-G/bit per data rates that will be available the real question now for CIOs and Line of Business Managers will be trying to decide what they want to do with the data that will be available to them. CIOs have to stop seriously considering whether they can bring the data to where the analytics is and instead take the analytics to the data.
There are some complexities here that have to be addressed, for decisions will be based on what the endpoints are like. Some will be smart in the extreme, while others will not only be dumb but will only need to communicate occasionally. There are, however, architectural developments emerging to accommodate these changes.
Make your data center ‘edgy’
For example, edge compute nodes are now starting to emerge. These are devices which aggregate local data and process it locally, only feeding relevant results, via the cloud, to regional or central compute services. This points towards distributed data center models where the data center itself can start being spread out into the cloud. This makes a good deal of sense, if only because of issues such as latency, by allowing the processing capabilities to be located much closer to the data sources.
There is now growing interest in having edge data centers in the cloud, much more localised, yet much closer to the enterprise, much more an integral part of the organisation. These edge data centers are still a virtual part of the enterprise environment rather than out in the public cloud, even if they are hosted remotely, so those CIOs who feel they need to retain sovereignty over their data can have that capability.
The communications underpinning this architecture will depend on 5G to provide the necessary bandwidth, and it is already being planned for and early implementations phases started in several areas of the world.
In theory, it could be possible for enterprises to redeploy and utilise legacy data center hardware in some of these edge data center roles. But in practice this is going to be less than likely. This, as Quach points out, is not because of processing deficiencies. More challenging from an IT perspective will be issues such as size and power consumption.
Power is always a concern though the architecture will spread its consumption pattern and delivery requirements. Space will be the bigger issue, however, he reckons:
You could get little shacks hosted at the edge of the network where you have big servers but require them in a small form factor and consuming low power. So we are doing more integration around the capabilities that are in a data center, where you start integrating I/O, storage and compute into smaller packages.
He also expects to see a lot of acceleration technology integrated in as well, especially as the need for AI and machine learning capabilities also becomes part of the move out to the edge to work with local data locally. This, he suggests, will be a far more efficient way of processing data.
Another new architecture
This will require a new software architecture to match its physical capabilities, with one of the key requirements being the provision of a high degree of autonomy in its operations. And because it is coming in from the cloud end where many of the applications will have developed, this new architecture will need to have a very similar cloud-oriented architecture.
Intel is in the process of developing new reference architectures and designs in this area in order to create some de facto build standards with a view to accelerating development. These are container-based and hold tested and documented configuration data for a generic server. The objective will be to create what he calls `the easy button’ for service providers or enterprises to push when it comes to building interoperable applications and services.
One of the company’s primary pitches at moving 5G technology and the edge data center movement along is a new high performance modem chipset that goes into phones and PC devices. It has been carrying out a number of broad interoperability trials with Huawei, Nokia, and Ericsson. These are designed to take advantage of the trend dis-aggregation and taking advantage of server architecture and economics. Quach says:
We have to work with a vague ecosystem of hardware and software vendors. And so we have this program called the Intel Network of Builders (INB) program. There are over 300 companies signed up in this program, including O/S vendors, orchestration vendors, hardware, software, and applications vendors, and we release these things called Intel Select Solutions, which are essentially pre-configured, pre-tested, pre-validated configurations for specific functions.
The object here, he says, is to help enterprises avoid getting locked into any single system vendor and can build interoperable environments from a range of system vendor offerings.
He did, of course, acknowledge that the key to this would be the use of a common Intel chipset.
My take
5G is now getting close to being a reality – indeed, an increasing amount of it is already. And what Intel is working up to here are the types of infrastructures, architectures and operational models that many enterprises will need to be employing as the next decade comes along. And the key here is that 5G will be capable of delivering huge gobs of data very fast, so CIOs really do need to be planning now about not just how they manage that level of data, but how they need to exploit that data, and where is the best place to conduct that exploitation. Get that right and real business value will surely follow. Getting it wrong – even by not doing it at all – could well prove terminal.
Image credit - @Jirsak, from Shutterstock.com.Read more on: Cloud platforms - infrastructure and architectureInfrastructureIoT robotics and AI 