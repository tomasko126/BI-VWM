
It’s been a busy year for cloud analytics vendor Birst, following the appointment of new CEO Jay Larson and securing $65 million in a new round of funding. Having followed Birst’s story over the past 12 to 18 months, it’s evident that the company is benefitting from Larson’s direction in terms of its go to market strategy.
Its USP has been clearly defined as the cloud application that knits together all of the siloed BI functions within an organisation, linking the datasets and allowing a broad set of users to easily access intelligence. It’s not trying to rip and replace legacy, it’s about creating a fabric of usable and scalable BI in the cloud.
Birst’s latest push is around the idea of ‘networked BI’, which Chief Product Officer Brad Peters describes as a ‘fundamental new concept’ in terms of how you think about and apply analytics across an organisation. In a meeting in London with me last week, Peters said:
So BI has historically been about silos, i don’t care if it’s legacy BI or if it’s the newer desktop discovery guys. Whatever, it’s silos. You take a finite dataset and put it together and it solves a very finite problem. It has gotten worse over the past couple of years as well with the advent of these discovery tools, because it makes it very easy to create a desktop silo.
But even with the big guys, you built a couple of data warehouses, but you still a bunch silos sitting all around.
What Birst does, and the principle behind networked BI, according to Peters, is that it pulls all of these silos together and creates something fundamentally more usable. He said:
The analogy I use is that you can think of LinkedIn as a giant contact book, it essentially has all your contacts and information in it. But it’s not terribly interesting as a contact book, is it? It’s interesting because the contacts are networked, there are pointers between all the contacts, and it’s that network which is what makes LinkedIn interesting.
Well the same thing should be true for an organisation’s data. They’re interesting in themselves, they are nice reference points for individual problem sets that you have. But what’s powerful is that if you can create pointers across all of those datasets, you can create a single cohesive data fabric that looks at all of the organisation’s data that someone can query. The system is smart enough to know how to traverse the data and answer questions.
It’s not just putting a bunch of data in a big database, which is how someone else would think about it. No, it’s all the routes between data are mapped at the application level. So a user, a non-technical user, can come in and say I want a bit of finance data, I want a bit of sales data, I want to match them together. Boom, you can create something new. That’s different.
A whole book
 However, Peters admits that not every organisation understands the benefits of linking silos and applying logic to these, bringing BI into production. Once again, the culture of a company can hold it back and getting people to give up control and work together is a challenge. He said:
I think some people can’t see past an individual chart or graph. And realise that the data behind it can tell thousands more stories if it was well integrated and properly understood. They view analytics as a collection of isolated pages, as opposed to do a book. It’s interesting having this conversation with people, it’s like describing colour to blind people.
They don’t get it. They see five charts and they think they have five discrete, unique, independent elements that have no bearing on each other whatsoever. The whole point of networked BI is that underneath that is a fabric of data that can tell 1,000 more stories. You have to understand that at the data level. If they don’t get that, if they view a book as collection of individual pieces of paper, we’re not going to have a happy ending there. So we have to move on.
Peters explained that it’s Birst’s multi-tenanted, cloud-based architecture that allows it to move away from siloed instances that don’t talk to or understand each other. He added that whilst the linking of data is relatively easy, it’s the multi-tenancy that’s ‘a bitch to get right’. Peters said:
Analytics is hard enough as it is, having a bunch of things in one environment that can scale, that can handle both batch processing and interactive processing, do it at variable points in time, support the speed of performance that end users want to see, but also allow people to do data prep and data management at the same time, turns out it’s hard.
Everyone that that has ever tried that, especially in the cloud, has come up with fairly compromised solutions that only do pieces of it.
Most people have been focusing on the last five, six, or seven years on how you store data and how you store data cheaply and how you process data. Hadoop was fantastic for that, but it doesn’t really solve the problem of how you consume it. Our solution looks at how do you model and create an environment where people can broadly consume this data you’ve stored.
The Tableau-style involves discovering an insight once and then you’re done. The core of analytics is actually not that use case. It’s how do I create a repeatable system that’s constantly delivering reliable and up to date information to a lot of people so that they can monitor business processes and take action if something’s not supposed to be?
Giving up control
Peters explained that in large organisations BI has historically worked by getting a central team to create a core source of truth, which is then pushed out to regional or organisations that sit around the edge of the centre. These fringe organisations then suck out the core data and build out their own datasets, manipulating them however they want. These data sources then all remain separate and disparate, with little knowledge visible or being shared across the organisation. He said:
You have this really interesting mish-mash, you would be taking the core data and pushing it out to the edges of the organisation.
This is how legacy BI works. A lot of legacy BI is about taking this really great centralised, trusted data that you have and pushing it out so that it gets used in completely untrusted ways across the organisation.
That’s very different than networked BI. If you have a multi-tenant environment, you can put in a single environment and you co-host all these different datasets. Either at a central level, at a department or regional level, or an individual level. Instead of pushing your core data out to the edges, you bring in from the edges that data into the core and let them link together. Now I can combine analytically the stuff at the core with the stuff that’s going on at the edges (e.g. manufacturing and purchased goods) so that you can get a sense of truth of how things function across the organisation. There’s now a consistency of data. But you can still do self service, because you can still let the individual markets supply their own data and build their own analysis.
Networked data is all about bringing data into this broader network.
However, despite the benefits, networked BI requires that the central team understand that a different model is needed.
It requires that the centralised folks give up on the model that they’re going to do everything. The progressive teams are perfectly happy to do that, because there are parts of this that they don’t want to do. Let other people do those things. Then they become facilitators. Once the core folk become facilitators to the broader organisation, then the other folks can have a dialogue and interact and support each other. It’s a newer model.
The older model was very much about hogging it all centrally, the only people that ever touch data are central and if you want something, sign up and we might get to you next year. That’s the model.
It’s possible that someone could combine some numbers that you don’t really want them to, or it’s possible that they could create a visualisation in a way that you didn’t want them to. But what if you say no? They are going to go and do it anyway, they’re just going to do it in Excel. It’s a certain amount of getting used to it being less tightly controlled, but as a result of giving that up, overall there is much more governance at the end of the day. At least now they’re coming to you and working with you. When they were sucking core data out to the edge, you might have had control of the core data, but you have zero control of the edge. Now you have reasonable control of the core and influence over the other stuff.”
My take
Birst’s messaging is getting clearer as time goes on. I prefer this ‘networked BI’ discussion to the two-tier discussion that came before it. At least this is talking about the outcomes of running with Birst.
Analytics is such a broad, diverse and complex discussion that it’s no wonder that enterprises don’t know what to buy into. What with Hadoop, NoSQL, the visualisation tools and the traditional players, it’s hard to know whether to start. However, Birst’s argument that it can tie all this together is quite appealing. Especially as it talks about making data usable for the average business user.
Now we just need more of those use cases to assess how successful this is in reality…
Read more on: Analytics planning and data analysis 