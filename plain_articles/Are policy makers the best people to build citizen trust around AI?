
Matt Hancock
Last week US tech entrepreneur Elon Musk called on US politicians to regulate the rise of AI before it’s too late, citing lurid visions of killer robots stomping down the street to grab attention. I said at the time that such imagery isn’t helpful, even if eye-catching.
What matters most with the AI and machine learning revolution is trust. It’s comparable in many ways to the emergence of cloud computing into the mainstream. One of the critical factors in making and sustaining the cloud model as a viable option is trust on the part of the users. Any breach of that trust and the tower comes tumbling down.
It was encouraging to then to see the UK Minister of State for Digital Matt Hancock picking up on the trust theme when he addressed the Leverhulme Centre for the Future of Intelligence (CFI) conference in Cambridge. Hancock’s premise that a vital starting point in discussions around AI needs to be ask why trust matters in public policy overall? He said:
The truth is this – without trust, public policy is incredibly difficult. At one level, any progressive politician, particularly of the centre-right, believes that our task is to unleash the potential of all citizens, to remove barriers where we find them, and to empower and help citizens to make the most of their lives. So the entire mission and purpose of government is founded on a profound trust of people’s capability, so a free society is a better society. Trust the people!
There’s also a practical as well as philosophical aspect to this question of trust, he noted:
It’s hard to see how citizens will back innovation in the public or the private sectors, and won’t be prepared to take decisions that enhance the collective good, without high levels of trust in others, and especially in those making decisions about them, whether businesses, or public services, or elected officials. So, innovation and progress depend on trust. Good policy improves trust, and trust in turn improves policy.
Hancock added:
Trust has to be part of how AI can develop to benefit society and the economy now, not an afterthought. Why is this important? Senior decision-makers will not let their organisations’ data be used for AI if they don’t believe the data will be used securely, and if they don’t believe AI can help them to deliver better. Employees will not work well with AI-driven functions if they don’t trust that those work better than what they have. Consumers and citizens won’t accept widespread use of AI if they don’t see questions of trust properly addressed.
None of this thinking is particuarly new, Hancock acknowledged – “So far, so ageless” – but he argued that AI brings new considerations to the fore:
I want to ask, how do we best ensure advanced digital technology enhances, rather than undermines, trust, and how can we generate the trust needed to harness advanced digital technology? In part thanks to technology, we are living in an age of scepticism, in which sources of authority are challenged in a way unprecedented. Rational scepticism is the basis of scientific progress; so how do we harness scepticism to generate, not undermine, trust in a wider sense?
Panglossian?
Hancock’s pitch on this is that while technology moves “ahead of the rules and norms we have” and that this can be “particularly tricky”, he thinks that AI and other advances need to be approached optimistically, adding the proviso that:
We mustn’t allow optimism to become panglossian.
That’s a very ‘one the one hand, but on the other hand’ political equivalence that plays both sides, but that’s only to be expected. More concerning is when this talk of leads to discussion of the potential harm that new tech can do. Or as Hancock put it:
 Digital technology is incredibly powerful, and with great power comes great responsibility. Just because you could, doesn’t mean you should.
At this point, it’s easy to anticipate where this line of reasoning is heading. Sure enough, it’s all about creating ethics, rules and regulations where “we balance freedom and that responsibility”. From here, it’s an easy next step to asking what the role of government needs to be and the introduction of some cliches, such as:
Freedom is precious, but not the freedom to do harm.
This should have a familar ring to it. It’s essentially the same reasoning that’s trotted out on both sides of the Atlantic by politicians seeking to regulate the internet and expand snooping powers for intelligence agencies. It’s the kind of  reasoning that ends up with confused and ill-thought out political posturing around encryption, for example.
Hancock pointed to the UK’s plans to introduce a new Data Protection Bill which will bring the European Union’s GDPR and Law Enforcement Directive into UK law, as a good example of the sort of “strong, effective regulatory regime” that he regards as vital. But that’s not enough in its own right, he added – there also needs to be anticipation of “future challenges on the horizon, before they are subject to the law and regulators”.
While that might start to sound a bit ‘Minority Report’, Hancock was in fact alluding to the need to try to anticipate new ethical and governance issues that could be triggered by AI, citing as an example:
Unfair discrimination will still be unfair. Using AI to make some decisions may make those decisions more difficult to unpack. But it won’t make fairness less important. This is not all about risks: some applications of AI could make it easier to detect unfair biases, including biases in the institutions and processes that we already use to make decisions affecting people.
He warned that in terms of governance, it’s likely that over-arching principles won’t be enough and that specific vertical and sectoral rules will be needed:
We don’t yet know how much we will be able to rely on general issues in AI and automation, and how much will be more specific to application areas. We may find that the solution to many challenges created by application of AI in particular sectors is to make sure that application of the existing sector rules can keep up, with the right tools, rather than that we need completely new rules for AI. We already have expertise in sector regulators. They may need to develop new skills to look at AI-related cases in their sectors, if the major businesses in their sectors use new data technologies, which many will.
Finally, there’s a need for policy-makers – and business people in the private sector – to get to grips with AI and not leave it to the techies. Hancock is unconsciously echoing Musk here – minus the killer robots – but the point is well made nonetheless:
The issue of who in society is able to earn and build trust, and how they do so, is increasingly complex. Politicians, policy makers and Parliament have a key role to play, but in reality we’re not always the only or best placed to earn trust.
That in turn led to a concluding rhetorical question from the Minister of State:
What do we know now about what drives or harms public trust in the digital age?
My take
I’ve been critical of Hancock’s response to crucial aspects of the digital revolution in society, not least his suppine attitude towards BT and its arrogant dominance of the UK network infrastructure that has done so much damage to the digital economy. But this was an interesting and considered speech that raised some good questions without resorting to The Terminator turning up in your back garden scenarios. The final point about policy makers getting their heads around AI and its implications – both good and bad – is the most important takeaway and, of course, the biggest ongoing challenge.

Image credit - Distrust - trust © Gajus - fotolia/TwitterRead more on: Digital government and public servicesIoT robotics and AIMachine intelligence and AI

