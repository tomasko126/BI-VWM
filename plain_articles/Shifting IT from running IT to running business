
IT should no longer be about running the IT, for now much of the work involved can be automated. That, at least, is the view of Sudarshan Srinivasan, director of product marketing at Nutanix, and it is, he suggests, where Nutanix is heading with its latest batch of enhancements:
A lot of tasks in a datacenter are spent on non-value-add tasks, just keeping the infrastructure up, rather than focussing on applications, services and business users – the things that drive business forward. IT is too important for IT professionals to be spending time dealing with infrastructure issues. IT is about enabling new business models, helping businesses compete better. It’s not about hypervisor upgrades. We want to elevate IT above always tweaking the plumbing.
The Nutanix aim is to add more automation to make operations easier and faster. To that end the Software Defined Datacenter (SDDC) is now the essential prerequisite to have and exploit.
The company’s latest, Version 4.6 iteration of its Acropolis software incorporates new developments to help admins reduce, and maybe off-load those tasks. All of them are software-based, so there is no change required in any hardware, either the company’s own appliances or those from partner vendors, such as Dell and Lenovo.
One focuses on improvements in performance, and another provides new applications mobility, mainly by exploiting the hypervisor agnosticism introduced last year with the arrival of the Acropolis system to now provide a one-click approach to changing hypervisors to suit the application requirement.
Perhaps the most important, however, is arrival of a new automated control fabric based on a new product called Prism Pro for workload optimisation and automation. The goal here is to provide one click automation from a single pane of glass. The aim is to provide one management and control environment for compute, storage and virtualisation, together with the ability for it to provide recommendations on how to fix issues as they occur:
It doesn’t have a fragmented view of the world and it enables staff to see everything from end-to-end. We believe this is the first time that users have been able to have a control panel that is truly unified in this way. It is also built on the same webscale technology as the rest of the system, so it is resilient and scalable to the same level as the underlying platform.
Its unified view of the world also feeds the new automation capabilities that remove the need for admins to write scripts to get individual components of a heterogeneous infrastructure to achieve specific objectives.
Every time there is any change, all these scripts need to be re-written, with the only short-circuit available being the opportunity to re-use existing scripts. But they still have to be loaded and run.
Invisible
There is also a brand new capacity planning tool based on a new algorithm with patent pending on it. The company has, for some time, had a tag line of ‘The Invisible Infrastructure’ and this does seem to move it a good deal closer to being able to deliver on that promise:
There are lots of algorithms out there to provide capacity trending information but each of them is only good for specific types of application, they don’t cover any application, so what we did with the new algorithm is pitch all these algorithms against each other and find the one that best fits a user’s application mix. So it is very accurate in predicting what the capacity needs are going to be.
So the system is able to make capacity recommendation across the board, from storage to compute resources – including the classic `kill those VMs that have not been used for the last 45 days’ scenario through to the less obvious, but very effective, step of moving workloads to resources within the Nutanix cluster that are more appropriate to their specific needs.
This uses a Google-like search based user interface so that admins can locate the functionality they are seeking on a contextual basis. He claimed this saved ploughing through the multi-click, hierarchical drill-down menus that are currently the only way to achieve a similar result. So with just one search argument, for example an application name, admins can be presented with all relevant contextual information about what they can do with or about that subject in the cluster available.
It does do fuzzy searches as well, so it is a very different way of looking at the world. It is now about what information admins need, not about what the system provides and the admins have to go and learn how to use.
The performance improvements being delivered are claimed to run from 100% upwards, regardless of the platform. They are targeted at improving the I/O speed, particularly with regard to storage.
It is not optimized for one platform and not another. What we have done is go back to the drawing board and asked how we can get better performance from the core I/O path.
The third main area of development has been in automating the ability to change hypervisors in Nutanix clusters with a single click. This compares to the current situation where changing hypervisors can involve many steps and months of planning.
Much of that current IT management `tweaking’, of course, goes on maintaining the health and well-being of established, legacy applications, but as a growing number of these are now certified to run in the Nutanix hyper-converged environment they are ripe to come under the umbrella of more automated support.
For example, the current accepted wisdom is that legacy applications with known performance and resource requirements are normally said to be best suited to bare metal environments, quite probably running in co-location service environments. That, according to Srinivasan, is now at the point of change:
Virtualization has been playing catch up on bare metal but now offers the same speed for almost all applications – even the likes of Oracle. Virtualisation and containers have reached the point where our customers are running Oracle in such environments with no degradation in performance. It also creates a single, scalable pool of compute resource. This allows you to do things that cannot be done if you have a silo, and that is what we are looking to eliminate. Bare metal is essentially a silo.
And with the growing trend towards collaborative environments between front-end web applications and legacy back ends, he suggests there a couple of reasons why the virtualised, hyperconverged environment is now the better option.
Firstly, the trend is now towards services built of front end, user-facing applications working with back-end systems by using APIs across an enterprise service bus, so the back-end systems are no longer working just in their isolated silo.
Secondly, virtualisation means that even back-end applications can be cheaper to run if and when they can be included as part of the compute resource running the front end applications rather than as a separate silo. So even if co-located bare metal is cheaper than traditional legacy datacentre resources, moving to the hyper-converged model now has both operational and cost advantages.
Virtualization is now becoming an integral part of the environment – a feature – rather than an add-on product that has to be installed and managed separately, so it is now available for use everywhere at no additional cost, and it reduces the total cost of ownership because users can automate a lot of activities on the compute side.
My take
This is the beginning of a serious trend in IT, where the cost and performance advantages of automation outweigh the social implications of job `rationalistion’ to the point where it must be declared a `no contest’
Disclosure - At time of writing, Nutanix is a premier partner of diginomica. Read more on: DevOps NoSQL and the open source stackInfrastructure 