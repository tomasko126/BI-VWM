
I can see you
Picture a world in which machine-based decision-making spreads and scales up to become the dominant force in a data-based, increasingly paranoid society.
It’s not a hypothetical scenario: not long ago, a friend of mine took a photograph of a friend at a railway station coffee stand.
For that, he was detained and questioned under the UK Prevention of Terrorism Act, and his name placed on a database of suspects. One photograph and one cup of coffee were all it took to make him a potential enemy of society.
Take the UK government’s data-retention rules (recently found to contravene European law), and the so-called ‘Snooper’s Charter’.
Initiatives like these are the next logical steps for any government that places inordinate faith in context-free data’s ability to reveal simple truths about complex human beings – even if its record of creating IT systems to deal with that data is shocking (The NHS National Programme for IT, anyone? Or Universal Credit?).
We live in a dangerous world; no one denies it. But these data-retention and snooping plans mandate the creation of yet more algorithms and machine-based systems to root out supposed bad behaviour, because it’s obvious that human beings won’t be able to monitor our communications, 24 hours a day.
My innocent friend with the banking problems which I cited in part 2 of this report – Digital Dystopia – when algorithms attack – knows what that’s like in the real world.
That political intention gives rise to many questions. Who will write these algorithms, and why? Based on what rules, what beliefs, and what presupposed evidence of wrongdoing? And what words might they search for in our once-private communications?
For example, could any such system tell whether a device, network, or cloud platform is being used at any one time by an adult, or by a minor? That’s easy: No.
And as anyone under 18 has a right to privacy under the UN Convention on the Rights of the Child – the most widely upheld statement of human rights in history – the UK government would have no choice but to create a national security exemption that allows them to intercept children’s communications.
It’s claimed that, by monitoring everyone’s communications, the ‘Snooper’s Charter’ will protect children from abusers and pornographers. It won’t; instead it will criminalise children in their thousands. Why? Because the vast majority of explicit images of minors are made by minors themselves.
Teenagers regularly send explicit images of themselves to each other (so-called ‘sexting’). If they’re under 18, that makes them sex criminals – even if they’re over 16 and in consenting relationships.
Scare-mongering, you say? Not at all. Rising numbers of teens, in legal, consenting relationships, are already on the Sex Offenders Register for sending pictures of themselves to their legal, consenting partners.
Telling the difference
Now, let’s suppose that such a machine-based system is also designed to look for evidence of extremist religious views, for example, via trigger words or phrases. Could such an automated, crude (and itself ideology-based) process distinguish between an extremist using a key word and, say, a historian, student, journalist, economist, researcher, international affairs worker, trainee priest, or simply someone who’s reading a book?
The experience of my unfortunate friend suggests that the answer to that question is very clear too: No.
Sexting
Consider this: The databases and machine-based systems that govern banks, the public sector, utilities, and countless other industries have been built and tested over decades.
They interface with each other and with the credit reference agencies – those supposed beacons of truth – and yet even today are unable to distinguish between an innocent man and an absconding debtor – as highlighted in part 2 of this series . 
After all, in the example I cited previously, the combined findings of their supposed intelligence and the billions of dollars of IT investment behind them, amounted to “has similar name” and “has moved quite recently”. That was all it took to destroy his financial reputation – and perhaps the lives of others who share the same name.
So: altogether now. Let’s join the primary school kids in singing ‘The Algorithm Song’ that I shared in part one of this report. Because as we face the rise of machine-based decision-making in the institutions that govern our lives, we are all helpless infants.
My take
British politician and former Home Secretary Michael Howard couldn’t have been more wrong when he said, years ago, that “the innocent have nothing to fear”.
The innocent have plenty to fear – from machine-based decision-making, at least. Especially when it’s placed into the hands of ideologs, bureaucrats, and offshore investors.
While a Digital Bill of Rights might protect people should they wish to opt out of digital systems, a Citizens’ Right to Appeal against automated decisions, based on faulty data, is needed when they have no choice but to opt in. Such a national appeal should override the interests of any organisation that denies them basic services. Human society must step in and help.
More, organisations should stop using technology to turn their employees into robots by preventing them from using their intuition, judgement, intelligence, empathy, and skill. They’re human resources, serving other human beings. That’s what it’s all about.
And finally, it stands to reason that the Snooper’s Charter and related proposals in both the UK and the US, will be used to deny many innocent people a place in society, based purely on mistakenly applied, context-free machine logic.
As a society that used to value decency, fairness, and tolerance, we must oppose it.
Read more on: Analytics planning and data analysisData privacyFuture of workIdentityInfrastructureRegulationSecurity 