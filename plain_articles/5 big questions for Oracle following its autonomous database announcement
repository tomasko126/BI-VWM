
Oracle is revamping its database pricing model while at  the same time offering what it hopes will be seen as a more secure and robust cloud database for both OLTP and data warehouse workloads operating in the cloud. Phil Wainewright has the announcement details.
There wasn’t a huge amount of comment from the ‘usual suspects’ in the analyst community but then we didn’t have the opportunity to participate in the Q&A that followed.
As always with Oracle – and every other big enterprise software vendor – the devil is in the detail but nonetheless, we can take a stab at raising questions of interest to buyers. I’ll contextualize these as I go.
I believe that by moving away from consumption based pricing, Oracle may find this a tougher sell than the PAYG model of today, even though the deal comes with some attractive pricing messages. Many of us in the cheap seats have long argued that enterprise customers should expect consumption based pricing in exactly the same way SMEs consume services. The new model encourages waste, unless customers build in sufficient margin  for errors in calculating cost and savings. Today, Ellison is saying that it is likely difficult for buyers to forecast accurately with IaaS and PaaS deployments. As a way of offering a simple model, Oracle now asks customers to lay down an anticipated annual spend and use it which ever way works best for them. That works up to a point and I can see procurement officers being happy with that. But I still want to see those TCO calculations to verify the operational labor savings Oracle is anticipating.
Question: Does my annual commitment represent a hard and fast number? It appears so. The savings appear attractive but then every TCO case will be different. Will Oracle help me figure this out?
Wainewright thinks that the new model could deliver higher margins for Oracle in the long term, principally through the twin effect of scaling volume and ongoing automation. Oracle is certainly optimistic, tasking the top executive team to deliver $10bn in IaaS/PaaS revenues in the next five years at 30% margin.
Question: What happens when Amazon responds as it surely will, as will Microsoft and Google at some point? If you look at the plethora of solutions that Amazon in particular, but also Microsoft and Google, are churning out, then you have to ask whether the focus on price at the expense of Amazon’s rapidly growing market share and portfolio will be enough to keep all current Oracle customers on board. Check Amazon’s last earnings announcement for the impressive array of logos it was able to mention along with the sheer number of database migrations since 2016. And to put this into greater perspective, if Amazon continues to grow AWS at the current pace, then it will be showing revenue north of $17 billion by the end of this year. That revenue has to be coming from somewhere. Guess where? What’s more, as Amazon continues to scale, it has shown it is prepared to peg margins at around 22-24%.
Question: Does Oracle really have enough by way of cost leverage to keep customers happy on a pricing model that marches in lockstep with Amazon while at the same time achieving the stated margins upon which the top executive variable compensation is based?
Listening again to the livecast, it struck me that despite the benefits outlined in Ellison’s address, this is fundamentally a defensive move that focuses DBA attention on the cost of keeping databases up, running and secure. Ellison says the databases will not fail and will have 99.995% uptime or the equivalent of 30 minutes planned downtime in a year. This is a powerful statement, especially in light of occasional but often spectacular failures in Amazon regions. That should make it much harder for Amazon to make a good case for running OracleDB in its data centers for business critical workloads.
Question: who cares enough anyway, other than to get some additional degree of comfort over the management of their OLTP systems and possibly shave maintenance costs? Today’s focus is upon analytics and that is where Amazon Redshift is hurting and will continue to hurt Oracle. I cannot think of a single analysis service I’ve viewed in the last six months (and I’ve internally reviewed a LOT of those same systems, mostly tied to machine learning and predictive capabilities,) that are NOT running on Redshift. I can only think of one where there was an Oracle offering. Most others are saying Amazon (preferred) or Google (if you must.) So how will Oracle address this issue and find favor with developers who are building the new classes of analytic and AI-centric solutions? How will Oracle convince customers that it is fundamentally better to run internally developed AI-centric data analytics in Oracle’s Exadata cloud?
Question: One thing that wasn’t mentioned on the livecast that will be of interest to Oracle apps buyers. Does this latest development help Oracle provide more performant cloud applications? Will those customers benefit in some way from this? If so then how?
My take
Despite my questions, there is much to like about the new model.
Making this announcement before Oracle Open World is a smart move. Oracle now has time to hear what partners and customers think, corral the inevitable slew of questions and hone the model so that when Ellison stands up again to talk about this, he’ll have plenty of feedback upon which the company can both draw and respond. In short, this can be viewed as a subtly clever charm offensive.
Whether you believe this is defensive or not, smart buyers will work out how to make this work to their advantage while keeping a firm grip on budgets. But they will need to carefully review their contracts and work out the TCO under this model compared to where they are today alongside anticipated development for the next year at least.
Image credit - via OracleDisclosure - Oracle is a premier partner at time of writingRead more on: Cloud platforms - infrastructure and architecture 