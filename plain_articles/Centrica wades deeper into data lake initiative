

When Centrica announced its 2015 financial results back in February this year, chief executive Iain Conn was keen to showcase the role that data analytics is playing in the multinational utilities company’s efforts to service customers’ increasingly smarter, connected homes. As Conn told financial analysts:

We increased our capability this year in the field of analytics. We already have three million customers with access to our analytics and insight products in the UK and North America, and are targeting an increase to five million by the end of 2016. We have a good starting position in the connected home and believe it could become a material part of the Group by 2020.

In fact, customer-facing analytics are the tip of the iceberg and part of a much wider analytics drive at the company. This has seen it establish and develop a Hadoop-based ‘data lake’ over the past couple of years, which is designed to fuel a wide range of insight-driven projects and is based on Hortonworks’ distribution of Hadoop. 
So it was good to catch up with Daljit Rehal, strategic systems director at Centrica, at this week’s Hadoop Summit in Dublin, in order to get an update on how this massive project is progressing. He told us:

For us, the journey continues. Having established a data lake, primarily by catching enterprise systems data – from ERP, CRM and billing platforms, for example – we’ve now opened up access to data scientists and business analysts who were previously using tools like SAS. We’re helping them to adopt the data lake as their main source of data, which is our aim for the entire enterprise.

At present, he says, around 500 members of this ‘data community’ are using the data lake, with many coding in Python in order to create their own reports and dashboards, an approach that is proving a great deal easier than using other tools. But Centrica’s gone further than that, Rehal adds: 

We’re making good progress with our more operational use cases, some of which are now up and running. For example, field engineers now have mobile apps that allow them to get information on the customer they’re visiting, rather than just the task they’ve been asked to perform at that property. 
Before, it was a one-way mechanism, a matter of relaying data on the job to the engineer. With the data lake, they now know more about this customer’s history and satisfaction levels, plus they can capture data about the customer and feed that data back, so it propagates back through different systems, right back to the data lake.
In CRM, meanwhile, our marketing team hit upon the idea that, instead of cutting lists of who to target, what we should be doing is taking action every time a customer interacts with us, because that’s where the real opportunities lie. 
So the goal has been to shift to more real-time decisioning: each time we’re interacting with a customer, our customer services staff get alerted then and there to ‘next best actions’, because we recognise that it’s during these interactions that we should be campaigning, rather than through mailing out emails and letters in bulk. That’s all fed through the Hadoop data lake.

Defining
Many companies report that one of the biggest issue they face with getting a data lake up and running involves the definition of data, because these kind of information platforms don’t just hold more data than it was ever possible to store before. They typically hold a far wider variety of data, from a wider range of source systems than their predecessors – and, in some cases, those data volumes may include unstructured data. So how has Centrica stepped up to the metadata management (MDM) challenge? Rehal says:

We haven’t found MDM to be a challenge – but that’s because we expected it to be a problem right from the outset. It’s something we anticipated very early on.
What we’ve done is create our own internal solution to deal with data definition. We’ve turned the metadata problem into an internal crowdsourcing activity that’s managed through our own workflows. So when a piece of data arrives in our data lake, we check to see if it has a pre-existing definition. 
If it doesn’t, we kick off a workflow that asks various parts of the organisation to complete a definition for that item. That’s now being rolled out. Initially, we survived internally because we kept that crowdsourcing part restricted to the IT team, but we’re now at the stage where we can ask the wider business to contribute to the definition and labelling of data.

Today, the Centrica data lake is supported by 250 nodes, mostly 16-core machines, and can offer storage capacity up to 4 petabytes. It already ingests in the region of 300 gigabytes of data each day and is the largest cluster using HDP (Hortonworks Data Platform) in the UK, according to Andy Leaver, vice president of international operations at the software firm. 
Its implementation, Rehal adds, has relieved the burden on Centrica’s legacy data warehouse appliance systems, some of which the company is now looking to decommission as it increasingly offloads tasks onto the data lake.
But what he’s really proud of is the spirit of big data innovation that now exists in his team, as its achievements are increasingly recognised not just by senior executives at the company, but also by academics at the University of London’s Royal Holloway College in nearby Egham and by companies overseas that increasingly seek Centrica’s help and guidance in their own big data projects. Says Rehal:

We’re seen as a team that innovates, and a company that innovates, on a much wider playing field and that’s exciting and motivating for all of us.




Read more on: Analytics planning and data analysisDigital enterprise in the real worldUse cases 