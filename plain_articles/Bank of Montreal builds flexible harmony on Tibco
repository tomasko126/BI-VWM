
I make no apology for this being one of my recurrent hobbyhorses, but it is my contention that the best innovations come from a firm base of something standard, something that the users already now, understand and can work with. As cloud services spread through corporate networks, at an increasing pace there is much scope for innovation in both the applications that are possible and the business processes they the underpin.
But what that then requires is some stability somewhere in the chain, some known technology on which to hang all the new stuff. That, in a nutshell, is the story told by Jean-Michel Arés, Chief Technology and Operations Officer of the Bank of Montreal (BMO).
His story starts around 2010 when the bank adopted some new architectural principles, and the point is that the principles remain equally valid today:
By choosing the correct architectural engineering we future-proof, at least to an extent, a solution in investments. The technology platform itself is prone to change, but the fact that we’re moving to services and the fact that we’re moving to an integration bus and the fact that we’re building all these adapters, allows us to build a durable asset.
That asset is referred to as the Smart Core, which is based on fundamental constructs underneath the data models. So even with various adapters and services that may change, the core is going to remain the same, and to Arés that is the key. For example, one current change is a move to using Hadoop and a graphic model, and from his point of view it is just a change, or an addition. It is not a major rebuild.
When solid = flexible
Having the Smart Core gives the bank enormous flexibility in what they do. This also gives them the speed, and therefore the time, to experiment. For example, a business team might ask, ‘Can we try this?’ – where `this’ is a new process using business critical legacy applications.
Traditionally this would be a major engineering job involving eight months or more of possibly risky development to get it to the point where it works as a test case. Now it is a case where they can expect to have a working demonstration available in just a few days. Arés says:
The more complex a business gets, such as a universal bank for example, you really are not able to do that. Once you realise that, you draw the conclusion that you have to find an alternate path. That alternate path is to breed flexibility. That is where Tibco works and their integration platform is pivotal. In the bank, we’ve made a clear decision that some of the legacy systems that are 25 plus years old are perfectly fine and that’s ok.
In a way it is essential that it IS fine, for the business rules and the expertise that have been accumulated in these systems are core to the business. There are serious business risks in attempting to change those applications, but there can be serious business benefits when it becomes possible to take information from them and serve it up to customers in new ways, or derive new data – usually by combining that information with data from other systems – for some new, valuable purpose.
The key to the Smart Core at BMO is the Connector Grid. This allows the bank to manage all possible links between systems with the minimum number of connections. Arés used the example of a network of 10 systems connected point to point so that every system can communicate with every other system. Mathematically (n times n minus one over two) this requires 45 individual connections. Using Tibco’s integration bus, it can be done with just 10. So, as Arés observed, when `n’ is 1,000, it makes for a much simpler network.
In addition, it provides a normalisation for how systems are integrated. Instead of having different languages, it is done using one. It also reduces duplications of functional code such as sub-routines that can end up having slight variations for different applications. Arés explains:
Often they’re duplicated, and work slightly differently in different legacy systems, and you think, well this really should be done one way. If you determine that it should be done one way then you can actually take that logic and build it in to the service without having to say ‘when we have to do this function we have to go to this piece of code’.
From this comes the ability to apply the same approach to the normalisation of business rules or functionality that need to be performed in a similar way. A case in point would be authentication access control. Historically, that has been performed in every application, but according to Arés the arrival of the mobile phone as a customer client has changed the rules. If the objective is to create a unified experience for clients it has to be done in a very simple, secure and, above all, harmonised way.
When data sings in harmony
A third objective is the harmonisation of data and how the data that comes from 100 or more different systems can be pulled together as a coherent whole that meets all relevant checks and balances and does not breach regulatory requirements. This also had the side benefit of improving the productivity of BMO’s IT operations.
For example, the digitisation of cheque processing, somewhat forced on US banks by the US Government following 9/11, and the ability to offer customers a choice between digital and paper statements – with the former getting widespread customer approval – has had a prompt impact on the bank’s bottom line.
Saving the costs and workload of paper handling also came with functions such as on-boarding new customers, a process that used to involve them signing multiple documents. Not only was this process speeded up to the bank’s benefit, but also generated a significant plus in terms of customer experience.
Another area for productivity growth came through the way the simplified architecture of the Tibco-based Smart Core allowed the bank to not just retire legacy systems, but also re-examine their roles. Those that were still adding value to the business – both existing and new through the creation of new derived systems – could be readily identified, maintained or upgraded.
Then there was the rationalisation of the applications that came with the use of Smart Core, and for BMO that was seen particularly in the rationalisation of data warehouses.
Arés labelled himself as pleased with the Tibco relationship so far:
I think my team would definitely say that Tibco has been a great partner and their technology has worked very well. It’s delivered on a promise and obviously that’s critically important in a business like ours. I think with them we’ve built the foundation of something that we can really evolve from here. My view is that in the next five years it’s going to be quite remarkable in terms of the pace of change.
When it came to detail about those five future years, however, he was understandably more coy. For example, given the recent announcement of Tibco Labs and its work on Blockchain, an extension of the relationship seems obvious, But he would not go further than to state that it is published information that the bank is part of a consortium working developments in Blockchain:
I don’t think any bank has a large scale Blockchain communication but it’s a technology that has a lot of potential and I think we’ll see that over the next few years as some of the technical detail and business model details are ironed out. We’re going to be right in the middle of understanding those opportunities and when the technology is ready we’ll make sure that we can benefit from it.
This is an area where Tibco does have direct interest. Its Dovetail Project, operating within the new Labs, aims to produce tools for building Blockchain-variant-agnostic smart contracts, and this is an application area which would surely be of interest to BMO, especially in terms of the complex value chain in areas such as trade finance. Here, the ability to describe data lineage, where the source of each data element in a curated data set can be fully explained, is vitally important, concludes Arés:
It’s very important because people have to trust the facts, and to trust the facts you have to understand how those facts are generated. In banking that is actually a regulatory requirement. I think the same will apply to cloud platforms as well. No matter whether it’s cloud or in-house we’re going to have to explain the lineage of our data.
My take
Here is a good example of what the word `platform’ should really mean, at a couple of levels. It is about some technology on which a business can feel it can both stand its business, and develop it; even experiment with bits of it to meet new market trends or regulatory changes, and feel assured that the platform itself will not do anything perverse or duplicitous. And when the business is a bank then it can feel confidence that the customer experience it delivers will not generate suggestions of perverse or duplicitous behaviour. They might even enjoy their banking experience, but at least they need to feel they can trust it.
Image credit - BOMRead more on: Analytics planning and data analysisCloud platforms - infrastructure and architectureDigital enterprise in the real worldFinancial services and fintechInfrastructureUser experience 