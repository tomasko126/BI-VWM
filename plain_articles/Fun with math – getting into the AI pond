
The current fascination with Artificial Intelligence (AI) is older than many of us realize. I can easily trace it to the 1980s and the area surrounding MIT once dubbed ‘AI Alley.’ But without a lot of effort you can go all the way back to 1956 and the founding of Fair, Isaac and Company, the folks responsible for the FICO score, now more generally referred to as your credit rating.
Going back to 1956 might be a strain for some intellects. At the time computing was nascent with mainframe makers like Unisys, Sperry, Burroughs, Control Data, RCA, and oh, yes, IBM, competing for mainframe business. But it’s easier to contemplate AI if you focus on algorithms, formulas that can take various inputs like earnings, credit history, and the like, and spit out a score that tells a lender how credit worthy a customer is. The score then influences the amount of money one can borrow and the interest rate that would be charged during payback.
However a FICO score was calculated back in the day, it is decidedly computer driven today and it can serve as a stand-in for the many other algorithms that govern modern life. Algorithms are a way to take something that is at least hard to quantify, if not impossible, and render a number or score that can be used for all the things that scores are useful for. More is better of course, except in golf, but if everyone agrees on the rules of engagement, everything is fine, right?
Maybe.
Algorithms provide what are essentially probabilities that something will occur or how an individual will act based on the prior behavior of many, many prior examples. The logical sticking point is that humans are assumed by economists to be rational actors in the marketplace and therefore their scores are a reliable bet. Most of the time this assumption is valid but when it is wrong, or when an algorithm fails to account for enough parameters (not data points per se but different attributes) things can go sideways to say the least. It all comes down to that pesky human attribute called free will, which often trumps rational behavior.
This brings us to Algorithm Blues a short article in “The Talk of the Town” section by Sheejah Kolhatkar in the October 10 New Yorker, (with Trump on cover as Miss Congeniality, nice touch). The article briefly profiles Cathy O’Neil a former hedge fund quant with a Ph.D. in math from Harvard. O’Neil’s latest accomplishment has been to write a book titled, Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy.
According Kolhatkar the book details how,
The lives of ordinary people are being undermined by the widespread use of algorithms.
It has just been nominated for a National Book Award.
In O’Neil’s formulation,
Any time there was some messy societal problem that people wanted to wish away with a silver bullet, that’s when math modeling pops up.
As examples, there’s the recidivism model, which is used in prison sentencing. In a 2005 study more than 70 percent of released offenders were re-incarcerated But a report from 2011 by the Pew Center on the States, showed that the average national recidivism rate for released prisoners is 43.3%. Either way this counts as a societal problem but which is right?
Also, there’s a handy algorithm that evaluates teachers. As part of her research O’Neil learned from a school principal that some of the strongest teachers we getting poor performance evaluations and being forced to leave their jobs. Suspecting the models were not “non-robust” O’Neil contacted the Department of Education to ask if she could see the algorithms but the D.O.E. refused.
The point is that algorithms are less than perfect and that when we treat them as such we open ourselves up to all sorts of errors that the algorithms were initially developed to eliminate. This comes as we in the business world are beginning to celebrate the coming AI era with all of its promise of automating simple decisions so that humans can be redeployed to more challenging value-add activities. But many people are concerned that this will simply eliminate jobs.
To buttress this view consider the hype-cycle, an algorithm of sorts that lays out the trajectory of new innovations. First there’s a kind of euphoria followed by a distorted belief that the new moiety, whatever it is, acts like a Swiss Army knife, able to do many things beyond what the initial innovators intended. Understandably, euphoria is followed by disappointment and the sinking belief that the new widget can’t do anything. From this low point, actual practice takes over and users adjust to using the new thing for its originally intended purpose. Then angels sing.
As we begin the journey of adding algorithms to the tiniest parts of business processes, hoping for great results, it’s worthwhile to contemplate that algorithms can be wrong and that AI has artificial as the first word in its name for a reason. Also, too often innovations prove their worth in ROI, often involving cost cutting, especially for labor. But that’s also early in the hype-cycle.
Later on, an innovation is less likely to be evaluated on cost cutting than on new capabilities it makes possible. Cost cutting is a one-time thing while new capabilities are open-ended and can increase in value over time—a good reason innovations outlast their hype-cycles.
My take
Can algorithms, AI, and machine learning be useful in business and in our daily lives? Unquestionably. But there will be a natural period of experimentation as businesses grapple with what these things can really contribute to business. This period is not to be avoided and as usual, the early adopters—those who gain first hand experience with an innovation early—will be the ones who make the greatest gains. AI and all the rest are low cost and have low barriers to entry. All the more reason to get started.
Image credit -  © Pixelbliss - Fotolia.comRead more on: Machine intelligence and AIRobotics 