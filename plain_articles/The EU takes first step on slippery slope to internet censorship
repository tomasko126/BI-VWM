
The European Union (EU) has signed off on the first steps towards greater regulation of the internet with a vote to establish a universal set of video content censorship rules that companies like Facebook and Twitter would be forced to follow.
The ruling was part of revisions to the EU’s “Audiovisual Media Services Directive,” issued a year ago, tackling extremism and hate speech online.
The EU Parliament wil have to give the final nod for the proposal to become law, but it seems inevitable that this will happen. Vice-President for the Digital Single Market Andrus Ansip says:
It is essential to have one common set of audiovisual rules across the EU and avoid the complication of different national laws. We need to take into account new ways of watching videos, and find the right balance to encourage innovative services, promote European films, protect children and tackle hate speech in a better way.
Individual EU states have tackled the issue of online extremism in different ways. For example, Germany recently passed a bill that makes companies open to fines of up to $53 million if hate speech was not scrubbed from their platforms within 24 hours of being flagged. And an Austrian court ruled earlier this month that Facebook must delete hate posts about the leader of the country’s Green Party.
Meanwhile in the UK, the question of how to manage the rise of extermist content online has become a policy issue in the forthcoming General Election on 8 June. The Conservative Party has been particularly forthright in warning of heavy financial penalties for online content platform providers who don’t toe the line.
And following the appalling terrorist attack in Manchester on Monday evening, it’s now being suggested that anti-terrorism legislation will be rushed through to force co-operation from social media firms as soon as the election is over if the Tories win a majority as the current polls suggest.
According to reports in The Sun – an enthusiastic basher of all thing Facebook, Twitter etc – Technical Capability Orders would be put in place to allow the police and security services to insist that the likes of WhatsApp would have to remove all encryption from suspect messages themselves for the first time. WhatsApp messages were sent by the perpetrator of the car terrorist attack on Parliament in March in advance of the atrocity and police complained they were unable to see what they said without having the phone in their possession.
Complex
The timing of the vote came after Facebook documents, leaked to The Guardian, revealed how difficult the social network finds it to police its own audience of nearly 2 billion users. Monika Bickert, Facebook’s head of global policy management, told the newspaper:
We have a really diverse global community and people are going to have very different ideas about what is okay to share. No matter where you draw the line there are always going to be some grey areas.
This leads to some interesting policy decisions. For example, online threats against a head of a state, such as Donald Trump or Theresa May, would automatically be removed, but a threat against nprmal citizens are left live unless the threat being issued is judged to be “credible”.
On terrorist activities, the documents indicate that in one month last year Facebook moderators identified 1,340 posts that posed “credible terrorist threats”, but only removed 311.
The leak also provided an insight into how Facebook makes judgement calls on what consistutes a terrorist organisation, citing 646 terrorist leaders and their groups But there are the inevitable problems of interpretation here. For example, the Facebook documents designate the Free Syrian Army (FSA) as a terrorist group. But the FSA is recognised a legitimate anti-Bashir opposition force by various Western governments, including the US and the UK.
Posts celebrating terrorist attacks, groups and members must be removed and images of leaders of terrorist organisations must be deleted if they are posted without a comment or with a supporting one, but they can remain if the comment is felt to be either neutral or condemning. The guidelines state:
People must not praise, support or represent a member … of a terrorist organization, or any organization that is primarily dedicated to intimidate a population, government or use violence to resist occupation of an internationally recognized state.
Things that have been censored on Facebook include images of breastfeeding and female nipples in general – male nipples are fine apparently; “plus-sized women”; and burn victims. In most cases, these were errors that were subsequently corrected, but are indicative of the pressure that the firm’s 4,500 community managers are under.
Last year Facebook, Twitter, YouTube, and Microsoft signed signed up to a voluntary code of conduct in Europe, under which they agreed to review and remove content flagged as hateful within 24 hours. But according to a European Commission study, only around 40% of reported content has been removed within that time frame, rising to 80% after 48 hours.
My take
I’ve said before that this far too complex a matter to be left to politicians to tackle. The European Union’s first step down the censorship is part of a wider protectionist stance that would force non-EU broadcasters, such as Netflix, to produce 20% of their content in Europe. So whatever the official line, this isn’t just about social responsibility; it’s also about stacking the deck for European media firms.
In the UK, if the Tories win the election, it will be all-out war with the social media firms. But then if Labour wins, it’ll be pretty much the same story, given that some of the most vocal social media critics are part of the current main opposition party. Meanwhile in the US, the Trump administration maintains its stance that social media firms are not playing a big enough role in the war on terror.
The social media firms themselves do have to start taking more responsibility – this nonsense about not being media firms isn’t going to stand and there should be some urgent rethinking going on about that as a defence against being seen to be more proactive. Yes, the likes of Facebook and Twitter are between a rock and a hard place on matters like extremist content and hate speech, but if they allow politicians to take the public moral high ground then they’ll have to take what they get – and that’s not going to be good for society in the long run.
Image credit - Freeimages.com Read more on: Digital and content marketingDigital government and public servicesGoverning identity privacy and security

