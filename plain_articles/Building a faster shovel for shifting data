
Most new developments in storage services and capabilities are, in practice, stories about marginal improvements in performance or ease of use.
For many users the results will offer ‘benefits’ and ‘improvements’ in their operations with a tangible Return on Investment, but they can rarely be classed as really ‘exciting’ (except to the vendor’s CEO, natch) or game-changing.
There is, however, the US start-up, WekaIO, which looks as though it might well be worthy of that second accolade. The trouble is, the game I suspect it may change will depend upon a recently forged partnership with another vendor – HPE – and how that develops over time.
There are things that about that partnership which, with a bit of speculation, could easily make 2+ 2 add up to 43,297. But there are elements of that speculation which could actually make 43,297 the right answer. To see why, some background on WekaIO and what it offers is pertinent here.
So how does it work, then?
The company’s `secret sauce’ is its Matrix software, an NVMe-native, POSIX compliant parallelised file system that runs on commodity servers. NVMe stands for Non-Volatile Memory Express, an open logical device interface specification for accessing non-volatile, solid-state drive (SSD) storage that is attached via a PCI Express (PCIe) bus. The company claims this approach delivers the highest bandwidth and lowest latency performance to any InfiniBand or Ethernet-enabled GPU- or CPU-based clusters, scaling up to thousands of servers.
It operates its own RTOS (real time operating system) in Linux user space, and talks directly to the server’s SSDs, using its own memory management without relying on the Linux kernel to provide very low latency. For applications that don’t run on Linux, it provides an NFS interface, plus SMB for Windows systems. There is also native HDFS support for Hadoop applications.
Among the advantages the company claims for it is the fact that its software stack is not only faster and with lower latency, but can also be ported across different bare-metal, VM, containerised, and cloud instance environments.
As one of its most recently announced partners is Amazon AWS, this suggests it will be possible for users to mix and match cloud and on-premise operations such as cloud bursting to best meet their operational requirements.
The company also claims that the software typically uses only around 5% of allotted resources for its own requirements, leaving 95% available for application processing.
The target applications the company has in mind are the obvious candidates where that famous term, `big data’, applies. These include Electronic Design Automation (EDA), life sciences, machine learning, artificial intelligence, financial trading and risk management, media and entertainment work such as rendering and other image manipulation tasks, and general High-Performance Computing (HPC) where its parallelised capabilities are already finding traction.
To give some idea of the performance advantages being claimed by the company it references an autonomous car project, where a FlashBlade system supporting a single GPU server took six and a half hours to run a metadata `find’ process. The WekaIO system took two hours. Another reference points to a metadata ls command on a 1 million-file directory taking 55 secs with FlashBlade and 10 secs with WekaIO. Overall, the company claims a 7x improvement in delivering data to where it is needed.
The system also has fairly obvious affinities with certain hardware architectures and these are reflected in the partnerships the company has recently forged. For example, its potential in HPC applications, where accessing data is one of the key potential bottlenecks, has been recognised by the San Diego Supercomputer Center (SDSC). Here, joint work is planned to leverage WekaIO’s software on advanced HPC research and development projects that have the shared goal of meeting the scientific challenges that have been outlined by the USA’s National Science Foundation (NSF).
It also has partnerships in the coming area of hyperconverged computing, both with Super Micro and Dell, which in turn has a partnership with Nutanix, and is well-suited to data delivery to GPU-based systems that are now widely used as specialist big-data processing engines.
HPE+The Machine+HANA+WekaIO = hmmmmmm
With that background in mind it is now worth going back to the speculation concerning WekaIO’s partnership with HPE and where it might lead. A meeting with WekaIO’s CEO and co-founder, Liran Zvibel, was met with an effective stone wall in answer to any questions on the subject. His desire to move as far away from the subject as possible is, as I expect will become obvious, totally understandable. But still the questions – and the connections that would seem to make them very plausible – continue to hang in the air.
The partnership is currently based on building on the plan to run WekaIO on big Superdome systems, which are amongst the favoured big data applications platforms used by large enterprises. But HPE and Superdome systems are, in that very context, one of the favoured platforms for running not just SAP applications, but SAP HANA-based applications. As HANA is fundamentally an in-memory processing architecture, the faster it can get data in the better it runs.
Zvibel confirmed that running HANA-based applications is a goal that the partners share, though the WekaIO system has yet to complete the SAP testing process for approval for running HANA applications. He was unable to give any date when that might happen, though one has to assume it will be in the first half of 2018, and probably first quarter.
The one trouble here, as pointed out in diginomica back in June, one of the issues with HANA is that its in memory processing architecture is somewhat stymied by only having old, Von Neumann-architect systems to run on. That is why HPE set out to create something more suitable. This now exists in very early prototype form under the gloriously portentous sobriquet of `The Machine’, which is based on a new processor chip architecture designed specifically for in memory processing.
The Machine, no doubt under some far more prosaic nomenclature, is not scheduled to appear any time soon – though some further announcements this year, that might give a hint, are quite possible. But even speculation would suggest that 2020 for some form of Beta-test machine for selected users to play with is a possible timeframe.
Now add HPE’s existing commitment to HANA, which could really exploit the hell out of an architecture specifically designed for it. Most of SAP’s applications portfolio runs on HANA already, and everything else important will as well by then. SAP is also an eco-system built around a wide range of partners that enhance, extend and implement core SAP services to match customers’ specific needs. So there is a strong sales and support infrastructure to back up such a development.
And finally there might just be WekaIO providing the data feed performance that such a system would require. After all, if HPE’s projections of The Machine working up at the Yottabyte level prove even close to reality no data management system currently in use is going to get even close. So something like WekaIO – or what it could become over the next few years – is going to be a necessity for any of this to work at all.
One can understand why Zvibel met all questions with such a stoic rictus. In many ways there is indeed `nothing to say’. But there is much to think about here and, for many an enterprise, much to plan the planning for. If it happens, it could happen rather quickly.
My take
Like all speculation there is, at this point in time, as much chance of it being an outrageous folly on my part as being bang on the money. Naturally I feel it will be the latter, and as a consequence I well appreciate Liran Zvibel’s reluctance to go within a hundred miles of the topic. But if it is bang on the money this could be a major game changer – at least at the enterprise and Big Data-consuming ends of the marketplace.
The Von Neumann architecture was developed back in 1945. It has done valiant service, and still does. But its core structure of `processor in one place and data in another place’ has the fundamental bottleneck of the join between them. Much has been done to expand the capacity of that join, but it soon fills up and becomes a bottleneck again.
And when the rate of data growth is set together with the growth in new ways that data can be exploited, that bottleneck will become a killer of processes and applications. It is possible to suggest it is time for something new. Whether this is it is, for now at least, speculation. But it may not be speculation for very much longer.
Image credit - Freeimages.com/Ronnie SatzkeRead more on: Analytics planning and data analysisCloud platforms - infrastructure and architectureInfrastructure 