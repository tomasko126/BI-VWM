
Keeping a grip on all its transactions is, quite understandably, a major undertaking for any bank and Italian-based bank UniCredit Group is not different. It has a transaction monitoring platform capable of logging some 8 billion, 2kbyte transaction events of all types each day.
But it is fair to say that, as with any bank, UniCredit had become well aware that there are transactions, and then there are transactions. The latter typically include those generated by the business community, especially the larger enterprises, where the volume can range from hundreds of thousands of small ones per day, through to just a handful of extremely large ones. Either way, the result always totals a very large monetary value. Messing up here can be expensive, reputation-damaging and, quite probably, career-limiting.
What is important here is the ability to monitor the business processes themselves, rather than just the operation of the technology. As Markus Sprunck, the bank’s Senior IT Architect pointed out at the recent Splunk .conf2016 Convention in Orlando’s DisneyWorld, Florida, just because the machinery processing a transaction works it doesn’t  mean it has actually processed a payment. The payment may have a wrong setting or problems in the infrastructure which cannot be detected with the classical monitoring.
So Sprunck and his team have built a dedicated Business Process Monitoring platform using Splunk, not so much as a monitoring tool but as an application development environment: 

The operations installation monitors almost all applications that we have in UniCredit. The business process monitoring application, however, picks just some out such as all of the information about payments, for credit transfer, cards and direct debits and foreign payments and high value payments. This is not a lot of data, but we need high quality. We talk about you seeing the tree in the trees, we are picking fruits from some trees but they are high quality. This means, for instance, that we don’t fetch data from block files, we do it with database connections. so we have connections to six applications and 90 source types where we fetch this information and then put indicators on it. 

The results are then shown in dashboards – which the team refer to as `tube maps’ after the London tube map. The interesting thing here is that they don’t use Splunk as a big data mining tool to find the classic anomalous needle in the haystack. It is more a case of analysing a relatively small amount of data – maybe just 100 Mbytes per day – in significant depth.
Where ‘signal-to-noise’ means a clear picture
A phrase widely used at this year’s .conf2016 concerned Splunk’s ability to improve the ‘signal to noise ratio’ of valuable information to `chaff’ in the monitoring of log data. Sprunck suggested this was exactly the goal here and he claimed the system has “almost no noise”. He explained: 

So for instance we have maybe 4,000 credit transfer baulks – a baulk is a kind of bunch of transactions from the same account. If one is missing, the traffic light goes red and then we have an escalation in the bank, somebody has to react, to do something. In a usual Splunk use case you have some kind of distribution of incidents or warnings where one may not be that important. This is different.
To achieve this, we don’t use Splunk as a tool, we use it as a development platform. Here I’m really impressed. I have a track record of 20 years’ experience in Enterprise development with C++, Java databases and all this stuff. We wrote an application based on Splunk using a test driven approach. At the moment we have 600 integration tests for this application and we’ve worked with a team of on average about 6 developers over one and a half years. 
So this has helped us to achieve high quality and I’m really impressed about the productivity when you develop with Splunk.  So if you look at the Java Enterprise development, you build more or less everything from a spreadsheet. Your business logic, your persistency, your user interface. You have to do a lot of non-productive work which is not a feature for the customer because you have to set up the environment in the base components and this is something that comes with Splunk.

He puts this down to the fact that Splunk is not a standard software development environment, but a tool that is built to be scaled. And this is necessary if there are terabytes of data every day. The effort for software developers to do the same in another technology- for example .Net or Java – can be very high because they can require a lot of infrastructure work before actual development work begins.
Interestingly, Spruck has opted to avoid accumulating existing data collected by the system to create a richer dataset for analysis purposes. Instead they have implemented a calendar approach geared to what the expected workloads are going to be for the end user. The calendar is then used as part of the analysis process, identifying when workloads are expected to peak, for example month ends and the run up to periods like Christmas, and when it will be low, such as Bank Holidays. This is seen as a preferred option to using machine learning approaches, which can take, in his view, two or three years for a machine to learn such a pattern.
The business justification behind this is quite straight forward, for it is driven by what ultimately drive most businesses – keeping the cash flowing. Almost inevitably, this is particularly important to large enterprises: 

If you are doing a credit transfer as an individual and it doesn’t work for some reason you will complain, but just you complain, and it is usually sorted out after a short time. If a company complains and they are processing, per day, 1 million transactions that is something else entirely. And companies can submit bunches of data, huge files, maybe with 10, 20,100 thousand transactions, quite late.

For customers, later equals better
The bank has to make sure that this is processed within one day wherever possible. But for the enterprise customer there is a benefit in having a late cut off on the submission of the transaction data. Quite often this means submitting the file before 11am. But if the bank can say it is ok to delay submission until 1p.m. this can be a big business advantage. But to give them this advantage the bank has to ensure that it fully understands the complete process and the systems running it, said Sprunck:

To understand this, you need the data, so if you do the business activity monitoring you can see what data comes in and when, and then you can do an analysis and agree to a customer having a later cut off. And if something does go wrong, we can call the customer and say ok, you submitted 1 million transactions and we’re quite sure that we can process 500,000 of them but maybe the other 500,000 of them will be processed tomorrow morning. If you inform them before it happens the customer says thanks for the information – it’s not nice but it’s ok. But if you call them a day or two later, or if the customer calls you and asks: `where are my transactions?’- and then you say ‘I’ll call you back two hours and give you the information’ – This is not good.

This approach allows the bank to build dynamic schedules for payment processing, and schedule workloads because it knows in advance that on a certain day a company is going to come in with a peak workload, or a reduced workload. It means they can increase resources proactively, or when there is some form of customer emergency. 
Another benefit is better control for the outsourcing platforms. The bank is obliged by law to monitor the performance of its outsourcing partners. The team can also get some KPI’s out of the system, in real time that keep them informed on the State of the service. Before this project the team had to rely on the data that it got from others.
My take
This is an interesting twist on the Splunk story, taking it from its basis as a log monitoring tool to become an application development environment. Yes, it is a rather specialised application, but then again the ability monitor critical processes in real time is a role that may well strike a chord with many other enterprise IT departments. 
Image credit - UniCreditGroupRead more on: Cloud platforms - infrastructure and architectureDevOps NoSQL and the open source stackDigital enterprise in the real worldFinancial services and fintech 