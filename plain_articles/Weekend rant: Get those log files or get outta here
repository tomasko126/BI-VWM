
New Relic Insights screen
I’m not going to name names because I discovered something this last week that truly horrifies me and it is not uncommon.
There seems to be this unconscious assumption that because you’re using cloud services and/or you’ve outsourced your infrastructure to Amazon, Google or Microsoft Azure that you can simply forget about it. After all, they’re running the show. Right? Wrong. As our systems get more and not less complicated, even if, on the surface they seem simple, there will always be points of failure. That’s the nature of computing. And so it was that over the last week I started trawling around some of the sites that interest me and to my horror I found that very few have obvious signs of implementing a coherent approach to monitoring their applications. That’s where the seemingly boring topic of application performance management and log files comes in.
In fairness, it is perfectly possible that many of the sites I was looking at have yet to make the AWS/Google/Azure jump. But some have. It may also be the case that those same sites prefer to have their monitoring one behind the firewall rather than use something like Splunk’s SaaS service, AppDynamics or New Relic’s offerings. And then it happened.
We were attempting to run a convoluted email outreach that ties to some nice graphics that have embedded links to a form of workflow that makes the process look pretty and intuitive. At least that is the theory. The services we use live outside of our site. So – if anything goes wrong then we need to understand where the point of failure arose. One service told us we were getting updates, while another remained ominously silent. Now – the way this works is such that users can’t submit the form unless they provide a specific piece of information. Ergo they must complete the form before the service touches the other service.
We found that there were intermittent failures. The data we had gave us no clue as to what had happened. We know some of our users personally so got them to repeat the process in the hope there was some sort of edge case glitch. No such luck. We then got a small number to run the routine again, providing us with screenshots of what happened along the way. They confirmed what we suspected. The first part of the routine worked but the data was not going over to the second (and vital) service.
We relayed the situation in some detail back to the initiating service provider. They came back with a bullshit answer that demonstrated they’d not read the email properly. We sent them screenshots and the like at which the L1 guys gave up and passed it over to the developers. At that point I wanted to know from the log files whether there was a successful ping to the next service. They took a bit of time but eventually came back with a strange answer:
We don’t have the ability to provide that level of detail from our log files since the process involves parsing some really hefty AppEngine logs and requires a significant amount of resources to complete. That said, there’s a significant amount of error checking built in…
Yes – well that isn’t good enough. They go on.
Since the issues you’re seeing are exclusive to people who are already subscribed I suspect the issue might be related to our API implementation.
Oops – so we have an API (yes, we know that – it’s how the integration works in the first place) but no, we don’t know WTF happened and haven’t got the means to discover it anyway. That sux mightily. Now, if this was a tiny business with a handful of developers then I’d maybe understand the problem. Small vendors are often in minimum viable product mode but that’s not the case here.
This reminds me of the many services out there that rely upon Twitter for their business model but for reasons known best to themselves, refuse to pay Twitter for access to the Gnip data. As a result, their ‘analytics’ are meaningless.
We’re living in a very fast moving world and while there is plenty of innovation out there, and relatively easy ways of tying solutions together (think Zapier), there can be no excuse for leaving customers with a dead end because the information necessary for tracing problems can’t be found or is deemed too difficult or expensive to discover.
Needless to say, we have ensured that our main service providers and the developer team have access to error detection, alerting and monitoring systems. I suspect our use of those types of service will only increase as we extend diginomica.
Image credit - New Relic Insights screen shotRead more on: Customer service managementInfrastructureIT service management 