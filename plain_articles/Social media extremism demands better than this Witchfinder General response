
Social media giants like Facebook, Google and Twitter are “shamefully far” from blocking illegal or dangerous content – and heavy fines, for both corporations and even individual executives, should be considered to make them shape up.
Such fines should be on the order of similar possible penalties currently being considered in Germany of up to 50 million euros for companies and up to five million for individuals.
Other measures, such as forcing firms like YouTube to pay policing costs to better protect the public, should also be on the table, so serious is their laxity – and lawmakers should even think about making failure to police user content a crime in itself.
Those are the views of the UK government’s Home Affairs Select Committee, which has just published its study into ‘Abuse, hate and extremism online’.
The Committee alleges that it found repeated examples of social media companies failing to remove illegal content when asked to do so. While noting how social media companies have created platforms used by billions of people to come together, communicate and collaborate which are used often by campaigns and individuals for positive messages and movements challenging hatred, racism or misogyny, it also claims that:
It is shocking that Google failed to perform basic due diligence regarding advertising on YouTube paid for by reputable companies and organisations which appeared alongside videos containing inappropriate and unacceptable content, some of which were created by terrorist organisations.
The study also says that advertisements for these innocent third parties regularly appear on YouTube videos created by supporters of terrorist and Far-Right groups.
And as an advert appearing alongside a YouTube video typically earns whoever posts the video $7.60 for every 1,000 views, says the Committee, that means:
Mainstream reputable companies, charity donors and taxpayers were inadvertently funding terrorists and their sympathisers.
The BBC has already been able to quote a series of apologetic statements from the ‘culprits’, such as Facebook UK, which states it has developed “quick and easy ways” for people to report content so it could be reviewed and, if necessary, removed, but still accepts blame:
We agree with the Committee that there is more we can do to disrupt people wanting to spread hate and extremism online.
In parallel, Google has issued a statement to show how it is tightening its advertising policies and enforcement, made algorithmic updates, and is expanding its partnerships with specialist organisations working in this field.
Ugly
The report is the latest chapter in a string of ugly stories about very ugly subjects, with the tabloids having a field day in terms of pointing out how government online content had more than once prefaced some obscene example of Daesh atrocity, or even worse.
One particularly nasty example: the report claims YouTube refused to take down a piece of anti-semitic video ordure on the basis it “did not cross the line into hate speech”.
Another one being picked up by the press today; Twitter claiming a cartoon showing Asian males abusing a semi-naked white woman while stabbing her baby to death hadn’t infringed its “hateful conduct policy”.
Now we’re all equally nauseated – let’s take a cold, hard look at what’s really going on here, away from the political rhetoric and vested-interest outrage of the Right Wing tabloids.
The Committee interviewed a string of industry patsies, headline-hungry politicians – and even a few people who’d genuinely had a think about the problem.
One of the the politicians who has made the most political hay out of all of this is the Committee chair herself, Shadow Home Secretary Yvette Cooper, who’s declared:
Government advertisements and major brands advertising is still being placed on inappropriate and hate-filled sites. As a result, Google and these organisations are still profiting from hatred.”
In 2016, the Government spent £3.9m advertising on YouTube, it’s worth knowing (according to the report).
Now clearly there are issue to be addressed here. We’ve talked about the major ad tech – as well as PR – problems, extremist visuals and propaganda present to Google subsidiary YouTube. It’s becoming more and more of an embarrassment to the company that quality brand ads for blue chips like Audi and L’Oreal can bizarrely pop up before some of this horrendous garbage.
As recently as March, we were told that the firm was “looking into” its policies and controls to combat the problem – but we’ve not seen much evidence of any proper controls being imposed as yet. That needs to change.
But the problem is that the MPs behind this report give almost no credit to the hard work the sector has put in to try and address the issue.
Former European policy manager for Facebook, Luc Delany, told BBC Radio Four’s Today programme this morning that MPs hadn’t bothered to look at more than ten years of social media company dialogue with both the police and successive Tory and Labour governments on the issue, stating,
[The report] bashes companies and gets a few big headlines for the committee, but the solutions proposed don’t really play out in reality.
Quite. And as Delany also points out, social media companies have to service hundreds of millions of users with billions of hours of content.
But the Committee doesn’t believe it’s the scale of content to be curated, but something much more suspicious – a whiff of right-wing Silicon Valley libertarianism, claiming:
We believe it to be a reflection of the laissez-faire approach that many social media companies have taken to moderating extremist content on their platforms.
We note that Google can act quickly to remove videos from YouTube when they are found to infringe copyright rules, but that the same prompt action is not taken when the material involves hateful or illegal content.
Social media companies rely on their users to report extremist and hateful content for review by moderators. They are, in effect, outsourcing the vast bulk of their safeguarding responsibilities at zero expense.
My take
While there is much to commend in the study, it’s hugely to be regretted that more opportunity was not taken to try and present a more balanced view rather than embark on a witch hunt.
Yes, social media companies are poor publishers – and they need to accept that is what they are. In their current state of denial, they have not as yet acquired the legal disciplines and hard lessons learned of conventional newspaper and magazine creators, for example.
But there’s a little too much in here about ‘nasty capitalists being rubbish at making people behave properly’ for me to take the swingeing countermeasures suggested that seriously. It’s political posturing.
Yes – take vile things off the Web.
But make the place you share what you do with the world your nanny, psychiatrist and judge – each and every day?
That’s an immature, simplistic stance that is built on ‘something must be seen to be done, this is something’.
It’s really not an acceptable solution.
I look forward to a more adult debate about these issues as soon as possible.
Image credit - KeepCalmGeneratorRead more on: Governing identity privacy and securitySocial 